\documentclass{beamer}

% Thème Beamer
\usetheme{Madrid}
\usecolortheme{seahorse}

% Packages utiles
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{hyperref}

% Métadonnées
\title{Géométrie des Données et Apprentissage sur Variétés}
\subtitle{Module 1 -- Introduction enrichie avec exercices}
\author{Mastère Spécialisé HPC-AI}
\date{\today}

\begin{document}

% Page de titre
\begin{frame}
  \titlepage
\end{frame}

% Sommaire
\begin{frame}{Plan}
  \tableofcontents
\end{frame}


%================= MODULE 2 =================
\section{Module 2 : Graphes et diffusion sur données}

\begin{frame}{Module 2 : plan}
\begin{itemize}
  \item Construction de graphes (k-NN, $\varepsilon$-graph, pondérés)
  \item Laplacien de graphe : définitions et interprétations
  \item Diffusion sur graphe, noyau de chaleur
  \item Équation de Poisson discrète : apprentissage semi-supervisé
  \item Courbure de graphe et topologie intuitive
  \item Exemples : classification digits, segmentation
  \item Applications modernes : NLP, bioinformatique
    \item TP: Classification semi-supervisée sur graphes (ex : digits).
\end{itemize}
\end{frame}

%================= Slide 1 : Introduction =================
\begin{frame}{Construction de graphes sur données}
\begin{itemize}
  \item Objectif : représenter les relations locales entre points d'un nuage de données.
  \item Méthodes courantes :
  \begin{enumerate}
    \item \textbf{k-NN graph} : chaque point est relié à ses $k$ plus proches voisins.
    \item \textbf{$\varepsilon$-graph} : relier tous les points dont la distance est inférieure à $\varepsilon$.
  \end{enumerate}
  \item Graphes pondérés : assigner un poids de similarité à chaque arête
  \[
    w_{ij} = \exp\Big(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\Big)
  \]
\end{itemize}
\end{frame}

%================= Slide 2 : k-NN vs ε-graph =================
\begin{frame}{Exemple : k-NN vs $\varepsilon$-graph}
\begin{itemize}
  \item k-NN : garantit $k$ voisins par point, graphe potentiellement asymétrique (symétriser si nécessaire).
  \item $\varepsilon$-graph : connecte tous les points à distance < $\varepsilon$, nombre de voisins variable.
\end{itemize}

\begin{center}
\textbf{Schéma schématique :}

\texttt{
Points : $\bullet$ \\
k-NN : chaque point relié à k voisins $\rightarrow$ lignes continues \\
$\varepsilon$-graph : lignes selon distance $\varepsilon$ \\
}
\end{center}
\end{frame}



%================= Slide 3 : Graphe pondéré =================
\begin{frame}{Graphes pondérés}
\begin{itemize}
  \item Utiliser des poids pour refléter la similarité :
  \[
    w_{ij} = \exp\Big(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\Big)
  \]
  \item Propriétés :
  \begin{itemize}
    \item $w_{ij} \in (0,1]$, $w_{ii}=0$.
    \item Capture la force de connexion locale.
    \item Symétriser si nécessaire : $w_{ij} = w_{ji} = \max(w_{ij}, w_{ji})$ ou $\frac{w_{ij}+w_{ji}}{2}$.
  \end{itemize}
  \item Préparation pour Laplacien, diffusion et autres méthodes spectral-based.
\end{itemize}
\end{frame}

%================= Slide 1 : Définition du Laplacien =================
\begin{frame}{Laplacien de graphe : définition}
\begin{itemize}
  \item Soit $G=(V,E,W)$ un graphe pondéré avec matrice de poids $W$ et matrice de degrés $D$ :
    \[
      D_{ii} = \sum_j w_{ij}
    \]
  \item Laplacien non normalisé :
    \[
      L = D - W
    \]
  \item Laplacien normalisé :
    \[
      L_\text{sym} = D^{-1/2} L D^{-1/2}, \quad
      L_\text{rw} = D^{-1} L
    \]
  \item Propriétés :
  \begin{itemize}
    \item $L$ est semi-définie positive.
    \item $\lambda_0 = 0$, vecteur propre constant $\mathbf{1}$.
  \end{itemize}
\end{itemize}
\end{frame}

%================= Slide 2 : Interprétation =================
\begin{frame}{Interprétation du Laplacien}
\begin{itemize}
  \item Analogue discret du Laplacien continu sur une variété.
  \item Diffusion / propagation sur le graphe :
    \[
      f^\top L f = \frac{1}{2} \sum_{i,j} w_{ij} (f_i - f_j)^2
    \]
    - Petit $f^\top L f$ $\Rightarrow$ $f$ varie peu entre voisins fortement connectés.
  \item Eigenmaps :
    \begin{itemize}
      \item Les vecteurs propres associés aux plus petites valeurs propres ($\lambda_1, \lambda_2,\dots$) capturent la structure globale du graphe.
      \item Utilisés pour réduction de dimension ou clustering spectral.
    \end{itemize}
\end{itemize}
\end{frame}

%================= Slide 3 : Schéma intuitif =================
\begin{frame}{Laplacien : intuition graphique}
\begin{itemize}
  \item Chaque arête du graphe agit comme un ressort entre points.
  \item Les valeurs propres et vecteurs propres du Laplacien capturent la \textbf{dynamique des flux} sur le graphe.
\end{itemize}

\begin{center}
\texttt{
$\bullet$---$\bullet$   $\bullet$---$\bullet$ \\
|            |         |            | \\
$\bullet$---$\bullet$   $\bullet$---$\bullet$ \\
Flux : les différences $f_i-f_j$ tendent à se réduire via diffusion
}
\end{center}

\end{frame}

%================= Slide 1 : Diffusion sur graphe =================
\begin{frame}{Diffusion sur graphe}
\begin{itemize}
  \item Soit un graphe pondéré $G=(V,E,W)$ avec matrice de transition
    \[
      P = D^{-1} W \quad \text{(random walk)}
    \]
  \item Diffusion d'une fonction $f$ sur le graphe :
    \[
      f(t+1) = P f(t)
    \]
  \item Intuition : la valeur de $f$ se propage le long des arêtes en fonction de leur poids.
  \item Méthode symétrisée (pour propriétés spectrales) :
    \[
      P_\text{sym} = D^{-1/2} W D^{-1/2}
    \]
\end{itemize}
\end{frame}

%================= Slide 2 : Noyau de chaleur discret =================
\begin{frame}{Noyau de chaleur discret}
\begin{itemize}
  \item Noyau de chaleur : solution discrète de l'équation de diffusion
    \[
      H_t = e^{-tL} \approx \sum_{k=0}^{n-1} e^{-t \lambda_k} \phi_k \phi_k^\top
    \]
    où $(\lambda_k, \phi_k)$ sont les valeurs et vecteurs propres du Laplacien.
  \item Interprétation :
    \begin{itemize}
      \item $H_t(i,j)$ mesure la diffusion de la chaleur de $i$ vers $j$ après temps $t$.
      \item Les modes rapides (\(\lambda_k\) grands) décroissent rapidement.
      \item Les modes lents (\(\lambda_k\) petits) dominent pour grandes échelles.
    \end{itemize}
\end{itemize}
\end{frame}

%================= Slide 3 : Illustration =================
\begin{frame}{Illustration de la diffusion sur graphe}
\begin{itemize}
  \item Initialisation : chaleur concentrée sur un ou quelques noeuds.
  \item Après $t$ pas de diffusion : la chaleur se propage proportionnellement aux poids des arêtes.
  \item Permet d’extraire des structures multi-échelle et de mesurer la proximité entre points.
\end{itemize}

\begin{center}
\texttt{
Noeud initial : $\bullet$ chaud \\
Propagation : $\bullet$---$\bullet$ (valeurs augmentent) \\
Flux : les voisins proches reçoivent progressivement la chaleur
}
\end{center}

\end{frame}


%================= Slide 1 : Apprentissage semi-supervisé sur graphe =================
\begin{frame}{Apprentissage semi-supervisé : idée générale}
\begin{itemize}
  \item Objectif : propager des labels connus sur un petit sous-ensemble de points vers l'ensemble du graphe.
  \item Notation :
  \begin{itemize}
    \item $X = \{x_1,\dots,x_n\}$ : points du graphe
    \item $L$ : Laplacien du graphe
    \item $Y_\ell$ : labels connus sur un sous-ensemble $\ell \subset \{1,\dots,n\}$
    \item $f$ : fonction de labels à propager
  \end{itemize}
\end{itemize}
\end{frame}

%================= Slide 2 : Équation de Poisson discrète =================
\begin{frame}{Équation de Poisson discrète sur graphe}
\begin{itemize}
  \item Formulation :
    \[
      L f = 0 \quad \text{sur les points non étiquetés } u = V \setminus \ell
    \]
    avec conditions de Dirichlet :
    \[
      f_i = Y_i \quad \text{pour } i \in \ell
    \]
  \item Interprétation : trouver une fonction $f$ qui varie le moins possible entre voisins fortement connectés, tout en respectant les labels connus.
  \item Solution pratique : résoudre le système linéaire restreint
    \[
      L_{uu} f_u = - L_{u\ell} Y_\ell
    \]
    où $L$ est partitionné selon $u$ (non-étiquetés) et $\ell$ (étiquetés).
\end{itemize}
\end{frame}

%================= Slide 3 : Illustration =================
\begin{frame}{Propagation de labels sur graphe}
\begin{itemize}
  \item Graphe avec quelques points étiquetés (rouge/bleu)
  \item Résultat : labels propagés vers tous les points via Poisson discrete
  \item Intuition : labels se diffusent le long des arêtes pondérées
\end{itemize}

\begin{center}
	\texttt{
\bullet bleu     \bullet rouge \\
|  \ /  |          |  /  \| \\
\bullet ?       \bullet ?   (labels propagés) \\
}
\end{center}

\end{frame}

%================= Slide 1 : Courbure de graphe =================
\begin{frame}{Courbure de graphe : notion intuitive}
\begin{itemize}
  \item Concept inspiré de la courbure des surfaces continues.
  \item Mesure comment les voisins d’un noeud sont connectés entre eux.
  \item Idée :
    \begin{itemize}
      \item "Bottleneck" : points qui relient deux clusters → faible connectivité locale.
      \item Graphes très "tordus" → distances de diffusion plus grandes entre certains points.
    \end{itemize}
  \item La courbure discrète peut guider :
    \begin{itemize}
      \item La détection de communautés
      \item La compréhension des structures locales et globales
    \end{itemize}
\end{itemize}
\end{frame}

%================= Slide 2 : Topologie et diffusion =================
\begin{frame}{Topologie intuitive et diffusion}
\begin{itemize}
  \item Les graphes avec "bottlenecks" ralentissent la diffusion : la chaleur met plus de temps à traverser.
  \item Les vecteurs propres du Laplacien capturent cette topologie :
    \begin{itemize}
      \item Petites valeurs propres → modes lents → grandes structures
      \item Grandes valeurs propres → modes rapides → détails locaux
    \end{itemize}
  \item Exemple : deux clusters reliés par un petit nombre d’arêtes
    \begin{itemize}
      \item Distance de diffusion entre clusters > distance intra-cluster
      \item Permet de détecter naturellement les communautés
    \end{itemize}
\end{itemize}
\end{frame}

%================= Slide 3 : Illustration =================
\begin{frame}{Illustration : bottleneck et diffusion}
\begin{center}
\texttt{
Cluster A $\bullet$---$\bullet$---$\bullet$ \\
                  | bottleneck | \\
Cluster B $\bullet$---$\bullet$---$\bullet$ \\
Diffusion : chaleur traverse lentement le bottleneck
}
\end{center}
\begin{itemize}
  \item La structure topologique influence les distances de diffusion et le comportement des méthodes spectral-based.
\end{itemize}
\end{frame}

%================= Slide 1 : Classification digits =================
\begin{frame}{Exemple : classification de digits (MNIST)}
\begin{itemize}
  \item Construire un graphe k-NN ou $\varepsilon$-graph à partir des images.
  \item Pondérer les arêtes par similarité (ex. distance Euclidienne ou cosine) :
    \[
      w_{ij} = \exp\Big(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\Big)
    \]
  \item Appliquer Laplacian Eigenmaps ou diffusion sur graphe pour réduire la dimension :
    \[
      L = D - W, \quad L \phi_k = \lambda_k \phi_k
    \]
  \item Labels connus sur un petit sous-ensemble → Poisson discrete pour propagation
\end{itemize}
\end{frame}

%================= Slide 2 : Résultat embedding =================
\begin{frame}{Résultat : embedding spectral}
\begin{itemize}
  \item Les vecteurs propres associés aux plus petites valeurs propres de $L$ fournissent un embedding 2D ou 3D.
  \item Points proches dans l’espace réduit → images similaires (mêmes digits).
  \item Semi-supervised : labels propagés par Poisson discrete sur graphe pondéré.
\end{itemize}

\begin{center}
\texttt{
Exemple schématique : \\
0 0 0 1 1 1 2 2 2 ... \\
Clusters de couleurs indiquent labels propagés
}
\end{center}
\end{frame}

%================= Slide 3 : Segmentation d'image =================
\begin{frame}{Exemple : segmentation d'image}
\begin{itemize}
  \item Pixels = noeuds du graphe, arêtes = voisinage spatial et/ou similarité de couleur
  \item Pondération : 
    \[
      w_{ij} = \exp\Big(-\frac{\|I_i - I_j\|^2}{2\sigma_c^2} - \frac{\|p_i - p_j\|^2}{2\sigma_s^2}\Big)
    \]
    où $I_i$ couleur et $p_i$ position du pixel $i$.
  \item Appliquer diffusion / Poisson discrete pour propager labels initiaux (ex. superpixels, annotations)
  \item Résultat : segmentation cohérente en clusters homogènes.
\end{itemize}
\end{frame}
%================= Slide 1 : Applications en NLP =================
\begin{frame}{Applications modernes : NLP}
\begin{itemize}
  \item Graphe de mots ou documents :
    \begin{itemize}
      \item Noeuds = mots ou documents
      \item Arêtes = co-occurrence ou similarité sémantique
    \end{itemize}
  \item Pondération des arêtes par cosine similarity ou embedding pré-entraîné :
    \[
      w_{ij} = \frac{\langle v_i, v_j \rangle}{\|v_i\|\|v_j\|}
    \]
  \item Diffusion sur graphe pour :
    \begin{itemize}
      \item Propagation de labels (ex. classification de documents)
      \item Extraction de communautés sémantiques
      \item Représentations low-dimensional des mots ou documents
    \end{itemize}
\end{itemize}
\end{frame}

%================= Slide 2 : Applications en bioinformatique =================
\begin{frame}{Applications modernes : Bioinformatique}
\begin{itemize}
  \item Réseaux de gènes ou protéines :
    \begin{itemize}
      \item Noeuds = gènes / protéines
      \item Arêtes = interactions ou corrélations
    \end{itemize}
  \item Utilisation :
    \begin{itemize}
      \item Propagation de labels (fonction connue de quelques gènes → prédiction pour les autres)
      \item Clustering / détection de modules biologiques
      \item Analyse multi-échelle via diffusion et valeurs propres du Laplacien
    \end{itemize}
  \item Exemples : propagation de pathologies, analyse de single-cell RNA-seq
\end{itemize}
\end{frame}

%================= Slide 3 : Résumé =================
\begin{frame}{Module 2 : résumé}
\begin{itemize}
  \item Construction de graphes : k-NN, $\varepsilon$-graph, pondérés
  \item Laplacien de graphe : définitions, interprétations et Eigenmaps
  \item Diffusion et noyau de chaleur : distances multi-échelles, modes lents
  \item Poisson discrete : propagation de labels (apprentissage semi-supervisé)
  \item Courbure et topologie : bottlenecks, structures locales et globales
  \item Exemples pratiques : classification de digits, segmentation d’images
  \item Applications modernes : NLP, bioinformatique
\end{itemize}
\end{frame}

\end{document}


