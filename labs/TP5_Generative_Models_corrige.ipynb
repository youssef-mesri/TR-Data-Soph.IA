{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d18d02",
   "metadata": {},
   "source": [
    "## Exercice 1 : VAE sur MNIST — Solution\n",
    "On utilise PyTorch pour construire un VAE, entraîner sur MNIST et visualiser l'espace latent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Définition du VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(32, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(32, latent_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e172859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du VAE et visualisation de l'espace latent\n",
    "# (code d'entraînement et visualisation à compléter selon le dataset et l'environnement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19487a3d",
   "metadata": {},
   "source": [
    "## Exercice 2 : GAN sur MNIST — Solution\n",
    "On construit un GAN simple, on entraîne sur MNIST et on compare les images générées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=100, img_dim=784):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, img_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_dim=784):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(img_dim, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0898987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du GAN et visualisation des résultats\n",
    "# (code d'entraînement et visualisation à compléter selon le dataset et l'environnement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d11cff",
   "metadata": {},
   "source": [
    "## Exercice 3 : Flows — Solution\n",
    "On illustre une transformation bijective simple et on visualise la distribution transformée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d154b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# Transformation affine\n",
    "def affine_flow(x, a=2.0, b=1.0):\n",
    "    return a * x + b\n",
    "x = np.random.normal(0, 1, 1000)\n",
    "y = affine_flow(x)\n",
    "sns.histplot(y, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ec1b48",
   "metadata": {},
   "source": [
    "## Utilisation du dataset MNIST pour tous les exercices\n",
    "Les trois exercices (VAE, GAN, Flows) utilisent le dataset MNIST pour l'entraînement, la génération et la visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032d55ff",
   "metadata": {},
   "source": [
    "---\n",
    "### Exemple complet d'entraînement sur MNIST pour chaque méthode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b16ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement VAE sur MNIST\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dataset MNIST\n",
    "transform = transforms.ToTensor()\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "vae = VAE(latent_dim=2).to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "# Fonction de perte VAE\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Boucle d'entraînement\n",
    "for epoch in range(5):\n",
    "    vae.train()\n",
    "    total_loss = 0\n",
    "    for batch, _ in loader:\n",
    "        batch = batch.view(-1, 784).to(device)\n",
    "        recon, mu, logvar = vae(batch)\n",
    "        loss = vae_loss(recon, batch, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(loader.dataset):.2f}')\n",
    "\n",
    "# Visualisation de l'espace latent\n",
    "vae.eval()\n",
    "import matplotlib.pyplot as plt\n",
    "latents, labels = [], []\n",
    "for batch, label in loader:\n",
    "    batch = batch.view(-1, 784).to(device)\n",
    "    mu, _ = vae.encode(batch)\n",
    "    latents.append(mu.cpu().detach())\n",
    "    labels.append(label)\n",
    "    if len(latents) > 10: break\n",
    "latents = torch.cat(latents)\n",
    "labels = torch.cat(labels)\n",
    "plt.scatter(latents[:,0], latents[:,1], c=labels, cmap='tab10', s=5)\n",
    "plt.title('Espace latent VAE (MNIST)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement GAN sur MNIST\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "optim_G = torch.optim.Adam(generator.parameters(), lr=2e-4)\n",
    "optim_D = torch.optim.Adam(discriminator.parameters(), lr=2e-4)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    for real, _ in loader:\n",
    "        real = real.view(-1, 784).to(device)\n",
    "        batch_size = real.size(0)\n",
    "        # Train Discriminator\n",
    "        z = torch.randn(batch_size, 100).to(device)\n",
    "        fake = generator(z)\n",
    "        D_real = discriminator(real)\n",
    "        D_fake = discriminator(fake.detach())\n",
    "        loss_D = loss_fn(D_real, torch.ones_like(D_real)) + loss_fn(D_fake, torch.zeros_like(D_fake))\n",
    "        optim_D.zero_grad()\n",
    "        loss_D.backward()\n",
    "        optim_D.step()\n",
    "        # Train Generator\n",
    "        D_fake = discriminator(fake)\n",
    "        loss_G = loss_fn(D_fake, torch.ones_like(D_fake))\n",
    "        optim_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optim_G.step()\n",
    "    print(f'Epoch {epoch+1}, Loss D: {loss_D.item():.2f}, Loss G: {loss_G.item():.2f}')\n",
    "\n",
    "# Visualisation des images générées\n",
    "import matplotlib.pyplot as plt\n",
    "z = torch.randn(16, 100).to(device)\n",
    "fake_imgs = generator(z).cpu().detach().view(-1, 28, 28)\n",
    "fig, axes = plt.subplots(1, 16, figsize=(16,2))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(fake_imgs[i], cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Images générées par GAN (MNIST)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement d'un Flow sur MNIST (exemple simple)\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transform = transforms.ToTensor()\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "# On prend les images et on applique une transformation affine comme flow\n",
    "for batch, _ in loader:\n",
    "    batch = batch.view(-1, 784).numpy()\n",
    "    # Flow: transformation affine\n",
    "    a, b = 2.0, 1.0\n",
    "    batch_flow = a * batch + b\n",
    "    plt.hist(batch_flow.flatten(), bins=50, alpha=0.7)\n",
    "    plt.title('Distribution transformée par Flow (MNIST)')\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98146c14",
   "metadata": {},
   "source": [
    "## Exercice 4 : Modèle de diffusion sur MNIST — Solution\n",
    "On propose ici un exemple de code pour entraîner un modèle de diffusion simple (DDPM) sur MNIST, visualiser le bruitage et la génération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrigé : Modèle de diffusion simple (DDPM) sur MNIST\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transform = transforms.ToTensor()\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Réseau de bruitage simple (UNet simplifié)\n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 784)\n",
    "        )\n",
    "    def forward(self, x, t):\n",
    "        return self.net(x)\n",
    "\n",
    "# Paramètres de diffusion\n",
    "T = 100\n",
    "betas = torch.linspace(1e-4, 0.02, T)\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "# Fonction de bruitage\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "    sqrt_alphas_cumprod_t = alphas_cumprod[t].sqrt().to(device)\n",
    "    sqrt_one_minus_alphas_cumprod_t = (1 - alphas_cumprod[t]).sqrt().to(device)\n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "# Entraînement du modèle de diffusion\n",
    "model = SimpleUNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for x, _ in loader:\n",
    "        x = x.view(-1, 784).to(device)\n",
    "        t = torch.randint(0, T, (x.size(0),), device=device)\n",
    "        noise = torch.randn_like(x)\n",
    "        x_noisy = q_sample(x, t, noise)\n",
    "        pred_noise = model(x_noisy, t)\n",
    "        loss = nn.functional.mse_loss(pred_noise, noise)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# Visualisation du bruitage progressif\n",
    "x, _ = next(iter(loader))\n",
    "x = x[:8].view(-1, 784).to(device)\n",
    "fig, axes = plt.subplots(1, 8, figsize=(16,2))\n",
    "for i in range(8):\n",
    "    t = int(i * T / 8)\n",
    "    x_noisy = q_sample(x, t)\n",
    "    axes[i].imshow(x_noisy[0].cpu().view(28,28), cmap='gray')\n",
    "    axes[i].set_title(f't={t}')\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle('Processus de bruitage (MNIST, Diffusion)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
